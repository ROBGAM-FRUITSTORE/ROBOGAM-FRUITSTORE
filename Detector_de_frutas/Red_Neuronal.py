# -*- coding: utf-8 -*-
"""DETECTOR1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1utiA5xP9CUWE2OSo3UEwXQXpt_r8qg7c
"""

from keras.applications import VGG16
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
import scipy

#from google.colab import drive
#drive.mount('/content/drive')

# Dataset
Training_data = "MODELO/Dataset/TRAINING"
Testing_data = "MODELO/Dataset/TEST"

Training_images = 2000
Testing_images = 1000

# Parameters
Epochs = 15#10
Length, Height = 100, 100
Batch_size = 80#80, 50
Training_steps = np.ceil(Training_images / Batch_size)
Testing_steps = np.ceil(Testing_images / Batch_size)
Conv1_filter = 70#70
Conv2_filter = 100#100
Size_filter1 = (5, 5) #5-5, 3-3
Size_filter2 = (3, 3) #3-3, 2-2
Conv3_filter = 150
Size_filter3 = (2, 2)
Pool_size = (2, 2)
Classes = 10
lr = 0.01

# Preprocessing image
Training_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True
)

Testing_datagen = ImageDataGenerator(
    rescale=1./255
)

Training = Training_datagen.flow_from_directory(
    Training_data,
    target_size=(Length, Height),
    batch_size=Batch_size,
    class_mode="categorical"
)

Testing = Testing_datagen.flow_from_directory(
    Testing_data,
    target_size=(Length, Height),
    batch_size=Batch_size,
    class_mode="categorical"
)

# Load pre-trained VGG16 model without the top layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(Length, Height, 3))

# Freeze the convolutional layers in VGG16
for layer in base_model.layers:
    layer.trainable = False

# Create a new model on top of VGG16
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(24, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(Classes, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])

# Create data generators
training_generator = Training_datagen.flow_from_directory(
    Training_data,
    target_size=(Length, Height),
    batch_size=Batch_size,
    class_mode='categorical'
)

testing_generator = Testing_datagen.flow_from_directory(
    Testing_data,
    target_size=(Length, Height),
    batch_size=Batch_size,
    class_mode='categorical'
)

# Entrenar el modelo y almacenar el historial en la variable 'history'
history = model.fit(
    training_generator,
    steps_per_epoch=Training_steps,
    epochs=Epochs,
    validation_data=testing_generator,
    validation_steps=Testing_steps
)

#plt.plot(history.history['accuracy'], label='Training Accuracy')
#plt.plot(history.history['val_accuracy'], label='Testing Accuracy')
#plt.title('Model Accuracy')
#plt.xlabel('Epochs')
#plt.ylabel('Accuracy')
#plt.legend()
#plt.show()

model.save('MODELO/MODELO.h5')

